{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we don't do data balancing (i.e. 290 per label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/57ntql9x3lg7fsldp3xp6w1c0000gn/T/ipykernel_52391/3194682914.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled3 = df_filtered3.groupby('score', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "fpath = \"/Users/barrychen/Desktop/IELTSWritingHelper/datasets_ready/Task_Achievement.csv\"\n",
    "df3 = pd.read_csv(fpath)\n",
    "df3['score'] = df3['score'].round(1)\n",
    "\n",
    "df_filtered3 = df3[(df3['score'] > 3.0) & (df3['score'] < 12.0)]\n",
    "\n",
    "reverse_mapping_3 = {\n",
    "    3.5: 0, 4.0: 0,\n",
    "    4.5: 1, 5.0: 1,\n",
    "    5.5: 2, 6.0: 2,\n",
    "    6.5: 3, 7.0: 3,\n",
    "    7.5: 4, 8.0: 4,\n",
    "    8.5: 5, 9.0: 5\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_filtered3['score'] = df_filtered3['score'].map(reverse_mapping_3)\n",
    "\n",
    "# Find the maximum sample size for each class\n",
    "max_sample_size = df_filtered3['score'].value_counts().min()\n",
    "\n",
    "# Sample the maximum available size for each class\n",
    "df_sampled3 = df_filtered3.groupby('score', group_keys=False).apply(\n",
    "    lambda x: x.sample(len(x), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sampled3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some people say that to prevent illness and di...</td>\n",
       "      <td>Emerging disease is a complex matter as it IS ...</td>\n",
       "      <td>The candidate has effectively addressed the gi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nowadays celebrities are more famous for their...</td>\n",
       "      <td>In this present world,  famous personalities a...</td>\n",
       "      <td>The essay adequately addresses the task and at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>More people decided to have children in their ...</td>\n",
       "      <td>currently, there are more and more people make...</td>\n",
       "      <td>The essay generally addresses the task by disc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Many people believe that the current system of...</td>\n",
       "      <td>Once every month no private vehicles  a day ca...</td>\n",
       "      <td>The essay fails to address the prompt effectiv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some people think that instead of preventing c...</td>\n",
       "      <td>In today's time, human activities are having a...</td>\n",
       "      <td>The essay effectively addresses the given task...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9197</th>\n",
       "      <td>Men and women are different in terms of their ...</td>\n",
       "      <td>Owing to the different physical and mental abi...</td>\n",
       "      <td>The essay effectively addresses the prompt and...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9198</th>\n",
       "      <td>Some people think technology makes life comple...</td>\n",
       "      <td>There is no denying of the fact that for some ...</td>\n",
       "      <td>The candidate effectively addresses the given ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9199</th>\n",
       "      <td>In cities and towns all over the world, the hi...</td>\n",
       "      <td>For the past decades,traffic jam has been one ...</td>\n",
       "      <td>The essay effectively addresses the given task...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>Some people believe that teenagers should be r...</td>\n",
       "      <td>Doing voluntary jobs is one of many ways to co...</td>\n",
       "      <td>The candidate has adequately addressed the giv...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>Many customs and traditional ways of behaviors...</td>\n",
       "      <td>Some people argue that many traditional custom...</td>\n",
       "      <td>The candidate has effectively addressed the gi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9202 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0     Some people say that to prevent illness and di...   \n",
       "1     Nowadays celebrities are more famous for their...   \n",
       "2     More people decided to have children in their ...   \n",
       "3     Many people believe that the current system of...   \n",
       "4     Some people think that instead of preventing c...   \n",
       "...                                                 ...   \n",
       "9197  Men and women are different in terms of their ...   \n",
       "9198  Some people think technology makes life comple...   \n",
       "9199  In cities and towns all over the world, the hi...   \n",
       "9200  Some people believe that teenagers should be r...   \n",
       "9201  Many customs and traditional ways of behaviors...   \n",
       "\n",
       "                                                  essay  \\\n",
       "0     Emerging disease is a complex matter as it IS ...   \n",
       "1     In this present world,  famous personalities a...   \n",
       "2     currently, there are more and more people make...   \n",
       "3     Once every month no private vehicles  a day ca...   \n",
       "4     In today's time, human activities are having a...   \n",
       "...                                                 ...   \n",
       "9197  Owing to the different physical and mental abi...   \n",
       "9198  There is no denying of the fact that for some ...   \n",
       "9199  For the past decades,traffic jam has been one ...   \n",
       "9200  Doing voluntary jobs is one of many ways to co...   \n",
       "9201  Some people argue that many traditional custom...   \n",
       "\n",
       "                                                   text  score  \n",
       "0     The candidate has effectively addressed the gi...      0  \n",
       "1     The essay adequately addresses the task and at...      0  \n",
       "2     The essay generally addresses the task by disc...      0  \n",
       "3     The essay fails to address the prompt effectiv...      0  \n",
       "4     The essay effectively addresses the given task...      0  \n",
       "...                                                 ...    ...  \n",
       "9197  The essay effectively addresses the prompt and...      5  \n",
       "9198  The candidate effectively addresses the given ...      5  \n",
       "9199  The essay effectively addresses the given task...      5  \n",
       "9200  The candidate has adequately addressed the giv...      5  \n",
       "9201  The candidate has effectively addressed the gi...      5  \n",
       "\n",
       "[9202 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  count\n",
       "0      5    291\n",
       "1      4   1266\n",
       "2      3   3364\n",
       "3      2   2257\n",
       "4      1    694\n",
       "5      0   1330"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts_df = df_sampled3[\"score\"].value_counts().reset_index()\n",
    "value_counts_df.columns = [\"score\", \"count\"]\n",
    "value_counts_df = value_counts_df.sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'essay', 'text', 'score'],\n",
       "    num_rows: 9202\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch=16, lr=2e-5, epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nickmuchi/distilroberta-finetuned-financial-text-classification and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664156c78ffe43a991f61210be864290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9200 [02:30<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train loss 1.4157, val loss 1.3975, val f1 score 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: train loss 1.3175, val loss 1.3541, val f1 score 0.4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: train loss 1.2456, val loss 1.3143, val f1 score 0.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: train loss 1.1555, val loss 1.3505, val f1 score 0.4361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: train loss 1.0694, val loss 1.3098, val f1 score 0.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: train loss 0.9667, val loss 1.4227, val f1 score 0.4597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: train loss 0.8566, val loss 1.5195, val f1 score 0.4532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: train loss 0.7557, val loss 1.6378, val f1 score 0.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: train loss 0.6589, val loss 1.8191, val f1 score 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: train loss 0.5693, val loss 1.8616, val f1 score 0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: train loss 0.4804, val loss 1.9742, val f1 score 0.4515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: train loss 0.4185, val loss 2.1554, val f1 score 0.4398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     97\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ece1786/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ece1786/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ece1786/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_labels = 6\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"nickmuchi/distilroberta-finetuned-financial-text-classification\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"nickmuchi/distilroberta-finetuned-financial-text-classification\", num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate the input columns for each example in the batch\n",
    "    combined_text = [\n",
    "        p + \" \" + e + \" \" + t for p, e, t in zip(examples[\"prompt\"], examples[\"essay\"], examples[\"text\"])\n",
    "    ]\n",
    "    # Tokenize the concatenated text\n",
    "    return tokenizer(combined_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"prompt\", \"essay\", \"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"score\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Get the labels from the tokenized dataset\n",
    "labels = tokenized_datasets[\"labels\"]\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "# Store the indices for each label\n",
    "label_to_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "\n",
    "# Lists to hold the train and validation indices\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "# For each label, split the indices into train and validation\n",
    "for label, indices in label_to_indices.items():\n",
    "    # Shuffle the indices within each label to ensure random splitting\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split 80% for training, 20% for validation\n",
    "    split_idx = int(0.8 * len(indices))\n",
    "    train_indices.extend(indices[:split_idx])\n",
    "    val_indices.extend(indices[split_idx:])\n",
    "\n",
    "# Convert indices to tensors\n",
    "train_indices = torch.tensor(train_indices)\n",
    "val_indices = torch.tensor(val_indices)\n",
    "\n",
    "# Create Subsets for train and validation datasets\n",
    "train_dataset = Subset(tokenized_datasets, train_indices)\n",
    "eval_dataset = Subset(tokenized_datasets, val_indices)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize lists to track training/validation losses and accuracies\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Initialize F1 score metric (weighted-averaged for multi-class classification)\n",
    "f1_metric = F1Score(task=\"multiclass\", num_classes=num_labels, average=\"weighted\").to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    f1_metric.reset()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Record training loss for the epoch\n",
    "    train_losses.append(epoch_train_loss / len(train_dataloader))\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        loss = F.cross_entropy(logits, batch[\"labels\"])\n",
    "\n",
    "        epoch_val_loss += loss.item()\n",
    "        f1_metric(predictions, batch[\"labels\"])  # Update F1 metric with predictions\n",
    "\n",
    "    # Record validation loss and accuracy\n",
    "    val_losses.append(epoch_val_loss / len(eval_dataloader))\n",
    "    val_f1 = f1_metric.compute().item()\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: train loss {train_losses[-1]:.4f}, val loss {val_losses[-1]:.4f}, val f1 score {val_f1_scores[-1]:.4f}\")\n",
    "\n",
    "# Plotting function\n",
    "def eval_plot(train_losses, val_losses, val_f1_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    # Validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_f1_scores, label=\"Validation F1\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1')\n",
    "    plt.legend()\n",
    "    plt.title('Validation F1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot train loss, validation loss, validation accuracy\n",
    "eval_plot(train_losses, val_losses, val_f1_scores)\n",
    "\n",
    "# Print final validation accuracy\n",
    "print(f\"Final validation F1: {val_f1_scores[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
