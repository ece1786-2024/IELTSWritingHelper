{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial #4 (6 labels) not balanced data with different param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 5e-6, batch size = 16, epoch = 15, warm up = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\ece1786\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20838\\AppData\\Local\\Temp\\ipykernel_69576\\294484408.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../datasets_ready/Coherence.csv\"\n",
    "df5 = pd.read_csv(fpath)\n",
    "df5['score'] = df5['score'].round(1)\n",
    "\n",
    "df_filtered5 = df5[(df5['score'] > 3.0) & (df5['score'] < 12.0)]\n",
    "\n",
    "reverse_mapping_3 = {\n",
    "    3.5: 0, 4.0: 0,\n",
    "    4.5: 1, 5.0: 1,\n",
    "    5.5: 2, 6.0: 2,\n",
    "    6.5: 3, 7.0: 3,\n",
    "    7.5: 4, 8.0: 4,\n",
    "    8.5: 5, 9.0: 5\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_filtered5['score'] = df_filtered5['score'].map(reverse_mapping_3)\n",
    "\n",
    "# Sample the maximum available size for each class\n",
    "df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n",
    "    lambda x: x.sample(len(x), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dataset5 = Dataset.from_pandas(df_sampled5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'essay', 'text', 'score'],\n",
       "    num_rows: 8766\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53334d79a8345bc875bc7f0d9555113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\ece1786\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  4%|▍         | 119/2925 [2:31:26<98:49:19, 126.79s/it]"
     ]
    }
   ],
   "source": [
    "num_labels_5 = 6\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\", num_labels=num_labels_5, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate the input columns for each example in the batch\n",
    "    combined_text = [\n",
    "        p + \" \" + e + \" \" + t for p, e, t in zip(examples[\"prompt\"], examples[\"essay\"], examples[\"text\"])\n",
    "    ]\n",
    "    # Tokenize the concatenated text\n",
    "    return tokenizer5(combined_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets5 = dataset5.map(tokenize_function, batched=True)\n",
    "tokenized_datasets5 = tokenized_datasets5.remove_columns([\"prompt\", \"essay\", \"text\"])\n",
    "tokenized_datasets5 = tokenized_datasets5.rename_column(\"score\", \"labels\")\n",
    "tokenized_datasets5.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Get the labels from the tokenized dataset\n",
    "labels5 = tokenized_datasets5[\"labels\"]\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels5 = np.unique(labels5)\n",
    "\n",
    "# Store the indices for each label\n",
    "label_to_indices5 = {label: np.where(labels5 == label)[0] for label in unique_labels5}\n",
    "\n",
    "# Lists to hold the train and validation indices\n",
    "train_indices5 = []\n",
    "val_indices5 = []\n",
    "\n",
    "# For each label, split the indices into train and validation\n",
    "for label, indices in label_to_indices5.items():\n",
    "    # Shuffle the indices within each label to ensure random splitting\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split 80% for training, 20% for validation\n",
    "    split_idx = int(0.8 * len(indices))\n",
    "    train_indices5.extend(indices[:split_idx])\n",
    "    val_indices5.extend(indices[split_idx:])\n",
    "\n",
    "# Convert indices to tensors\n",
    "train_indices5 = torch.tensor(train_indices5)\n",
    "val_indices5 = torch.tensor(val_indices5)\n",
    "\n",
    "# Create Subsets for train and validation datasets\n",
    "train_dataset5 = Subset(tokenized_datasets5, train_indices5)\n",
    "eval_dataset5 = Subset(tokenized_datasets5, val_indices5)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader5 = DataLoader(train_dataset5, shuffle=True, batch_size=12)\n",
    "eval_dataloader5 = DataLoader(eval_dataset5, batch_size=12)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer5 = AdamW(model5.parameters(), lr=5e-6)\n",
    "num_epochs5 = 5\n",
    "num_training_steps5 = num_epochs5 * len(train_dataloader5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer5, num_warmup_steps=int(0.05*num_training_steps5), num_training_steps=num_training_steps5\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device5 = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model5.to(device5)\n",
    "\n",
    "# Initialize lists to track training/validation losses and accuracies\n",
    "train_losses5 = []\n",
    "val_losses5 = []\n",
    "val_f1_scores5 = []\n",
    "\n",
    "# Training loop\n",
    "progress_bar5 = tqdm(range(num_training_steps5))\n",
    "\n",
    "# Initialize F1 score metric (weighted-averaged for multi-class classification)\n",
    "f1_metric = F1Score(task=\"multiclass\", num_classes=num_labels_5, average=\"weighted\").to(device5)\n",
    "\n",
    "for epoch in range(num_epochs5):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    f1_metric.reset()\n",
    "    model5.train()\n",
    "\n",
    "    for batch in train_dataloader5:\n",
    "        batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "        outputs = model5(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer5.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer5.zero_grad()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        progress_bar5.update(1)\n",
    "\n",
    "    # Record training loss for the epoch\n",
    "    train_losses5.append(epoch_train_loss / len(train_dataloader5))\n",
    "\n",
    "    # Evaluate the model\n",
    "    model5.eval()\n",
    "    for batch in eval_dataloader5:\n",
    "        batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model5(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        loss = F.cross_entropy(logits, batch[\"labels\"])\n",
    "\n",
    "        epoch_val_loss += loss.item()\n",
    "        f1_metric(predictions, batch[\"labels\"])  # Update F1 metric with predictions\n",
    "\n",
    "    # Record validation loss and accuracy\n",
    "    val_losses5.append(epoch_val_loss / len(eval_dataloader5))\n",
    "    val_f1 = f1_metric.compute().item()\n",
    "    val_f1_scores5.append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs5}: train loss {train_losses5[-1]:.4f}, val loss {val_losses5[-1]:.4f}, val f1 score {val_f1_scores5[-1]:.4f}\")\n",
    "\n",
    "    fname = f\"CATEGORY_EPOCH{epoch + 1}_F1{val_f1_scores5[-1]:.4f}.pt\"\n",
    "\n",
    "    # Save the model's state_dict\n",
    "    with open(f\"saved_models/{fname}\", \"wb\") as f:\n",
    "        torch.save(model5.state_dict(), f)\n",
    "\n",
    "    print(f\"Model saved to saved_models/{fname}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Plotting function\n",
    "def eval_plot(train_losses, val_losses, val_f1_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    # Validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_f1_scores, label=\"Validation F1\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1')\n",
    "    plt.legend()\n",
    "    plt.title('Validation F1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot train loss, validation loss, validation accuracy\n",
    "eval_plot(train_losses5, val_losses5, val_f1_scores5)\n",
    "\n",
    "# Print final validation accuracy\n",
    "print(f\"Final validation F1: {val_f1_scores5[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as deberta_9k_ep15_4597.pt\n"
     ]
    }
   ],
   "source": [
    "modelsavename5 = \"deberta_9k_ep15_4597.pt\"\n",
    "\n",
    "# Save the model's state_dict\n",
    "with open(modelsavename5, \"wb\") as f:\n",
    "    torch.save(model5.state_dict(), f)\n",
    "\n",
    "print(f\"Model saved as {modelsavename5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barrychen/anaconda3/envs/ece1786/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/vx/57ntql9x3lg7fsldp3xp6w1c0000gn/T/ipykernel_8120/2724072977.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from deberta_9k_ep15_4597.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barrychen/anaconda3/envs/ece1786/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Specify the file name from which to load the model\n",
    "modelsavename = \"deberta_9k_ep15_4597.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "with open(modelsavename, \"rb\") as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n",
    "\n",
    "print(f\"Model loaded from {modelsavename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(input_text):\n",
    "  # Tokenize input and get model output\n",
    "  inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "  # Get logits (output for classification)\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "\n",
    "  # Convert logits to probabilities using softmax\n",
    "#   probs = softmax(logits, dim=1)\n",
    "\n",
    "  # Map classes to polarity values\n",
    "#   polarity_values = torch.tensor([-1.0, 0.0, 1.0])\n",
    "\n",
    "  # Calculate polarity score as the weighted sum of probabilities\n",
    "#   polarity_score = torch.sum(probs * polarity_values, dim=1).item()\n",
    "\n",
    "  # Get the predicted class (optional, for reference)\n",
    "  predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "  return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# task achieve: 6.5\n",
    "input_text = \"\"\"Prompt: Today people are surrounded by advertising. This affects what people think is important and has a negative impact on people's lives. To what extent do you agree or disagree?\n",
    "\n",
    "Essay: In this digital world, people are encircled with various types of advertisements. It is omnipresent for all, TV adverts, Social Media Marketing, billboards, Personal advertisements and many more diverse ways. while this phenomenon is escalating to extremely new level, people are in influence of it. In my view, it has more positive effects than harmful.\n",
    "\n",
    "Because of advertisements, people are aware of current products in market. At some extent, it educates the people and provides the knowledge. For an instance, In India serious disease like  polio is no more and the major success goes to awareness campaign help by famous personality with the use of digital marketing and TV advertisements. Moreover, to bind user with interest usually companies display various new idea and it add major value in entertainment world.\n",
    "\n",
    "However, the critical impacts are also not avoidable.Firstly, it exposes kids and young generation towards the violence and  inappropriate content sometime. Secondly, Advertisements with various discounts and offers, make people lure to do impulsive shopping. Increasing obesity is also one of consequence of advertisement of junk food. In addition, few times people get influenced by various advertisings and tend to work more to achieve never ending desire of buying stuff, this approach add up to the stress and frustration. What more, kids might suffer from harmful psychological effects when their parents are unable to afford various products shown in advertisements.\n",
    "\n",
    "To put this in a nutshell, I can say that advertisement is beneficial phenomenon with a number of insignificant drawbacks. In my view, negative advertising effects can be lowered with help of government using stringent approach and awareness campaigns.\n",
    "\n",
    "Comment: The candidate has adequately addressed the given task by discussing the positive and negative impacts of advertising on people's lives. They have provided relevant arguments and examples to support their claims. However, some aspects of the task, such as the impact on people's values and priorities, could have been explored in more depth. Overall, the candidate has demonstrated a good understanding of the topic and has fulfilled the requirements of the task.\n",
    "\"\"\"\n",
    "\n",
    "pred_class = get_class(input_text)\n",
    "print(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# task achieve: 8.5\n",
    "input_text = \"\"\"Prompt: Some people think that children should be taught at school to recycle materials and avoid waste. Other people believe that children should be taught this at home. Discuss both opinions and give your own opinion.\n",
    "\n",
    "Essay: Green energy is been regarded as the future energy for mankind, moreover,the fundamentals of achieving the same rely on reprocessing the waste. While some argue this should be educated at school level and others would like this initiative done from their homes. In this essay, we shall discuss both views and state my opinion.\n",
    "\n",
    "On one hand, schools have been institutional in developing children cognitive skills therefore all good behaviours are cultivated here. While waste management is a profound topic and requires an in-depth analysis of how the process works or implemented. Furthermore, tutors have authority over their pupils hence they pick up subject matters efficiently. Educational institute emphasis on reusing most of the resources at school, adding to this, student comprehend the reason and significance of reducing waste such as plastic and glass. For instance, Science as a subject, demonstrates the impact of plastic and non-biodegradable materials on earth’s soil, as these take up nearly million years to decompose, consequently converting these materials to toxic and destroying mother nature.\n",
    "\n",
    "On the other hand, young people often look up to their parents as inspiration to learn life skills. As they spend quality time at home, all behavioural attributes of kids are related to  them, so they should bring upon the habits of recycling waste at home. For example, when parents bring thumb rule of segregating waste at their homes, children at young age practice to keep waste as per the category and this behaviour can be carried across at various places like parks, social events, and public transport. As a result, children are instilled with the habit of re-cycling and segregating waste at an early age\n",
    "\n",
    "To conclude, education on recycling waste can be beneficial as it explains the impacts on the environment and what could be done to avoid this catastrophe. Also,parenting should ensure their children learn the habit of separating waste at home. In my opinion, both methodologies must be implemented to have a better future.\n",
    "\n",
    "Comment: The candidate effectively addressed the given task by discussing both opinions on whether children should be taught to recycle at school or at home. The ideas presented are clear, relevant, and coherent, with each paragraph focusing on one aspect of the task. All aspects of the task are adequately covered, with arguments and evidence provided to support each opinion. The candidate demonstrates a good understanding of the task requirements and fulfills them appropriately.\n",
    "\"\"\"\n",
    "\n",
    "pred_class = get_class(input_text)\n",
    "print(pred_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
