{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.copy_on_write = True\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_4628\\1543118313.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../datasets_ready/Grammatical.csv\"\n",
    "df5 = pd.read_csv(fpath)\n",
    "df5['score'] = df5['score'].round(1)\n",
    "\n",
    "df_filtered5 = df5[(df5['score'] > 3.0) & (df5['score'] < 12.0)]\n",
    "\n",
    "reverse_mapping_3 = {\n",
    "    3.5: 0, 4.0: 0,\n",
    "    4.5: 1, 5.0: 1,\n",
    "    5.5: 2, 6.0: 2,\n",
    "    6.5: 3, 7.0: 3,\n",
    "    7.5: 4, 8.0: 4,\n",
    "    8.5: 5, 9.0: 5\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_filtered5['score'] = df_filtered5['score'].map(reverse_mapping_3)\n",
    "\n",
    "# Sample the maximum available size for each class\n",
    "df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n",
    "    lambda x: x.sample(len(x), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dataset5 = Dataset.from_pandas(df_sampled5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd40784188a4ff0ade7dd38b098cf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26597\\miniconda3\\envs\\ece1786\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels_5 = 6\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\", num_labels=num_labels_5, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate the input columns for each example in the batch\n",
    "    combined_text = [\n",
    "        p + \" \" + e + \" \" + t for p, e, t in zip(examples[\"prompt\"], examples[\"essay\"], examples[\"text\"])\n",
    "    ]\n",
    "    # Tokenize the concatenated text\n",
    "    return tokenizer5(combined_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets5 = dataset5.map(tokenize_function, batched=True)\n",
    "tokenized_datasets5 = tokenized_datasets5.remove_columns([\"prompt\", \"essay\", \"text\"])\n",
    "tokenized_datasets5 = tokenized_datasets5.rename_column(\"score\", \"labels\")\n",
    "tokenized_datasets5.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Get the labels from the tokenized dataset\n",
    "labels5 = tokenized_datasets5[\"labels\"]\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels5 = np.unique(labels5)\n",
    "\n",
    "# Store the indices for each label\n",
    "label_to_indices5 = {label: np.where(labels5 == label)[0] for label in unique_labels5}\n",
    "\n",
    "# Lists to hold the train and validation indices\n",
    "train_indices5 = []\n",
    "val_indices5 = []\n",
    "\n",
    "# For each label, split the indices into train and validation\n",
    "for label, indices in label_to_indices5.items():\n",
    "    # Shuffle the indices within each label to ensure random splitting\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split 80% for training, 20% for validation\n",
    "    split_idx = int(0.8 * len(indices))\n",
    "    train_indices5.extend(indices[:split_idx])\n",
    "    val_indices5.extend(indices[split_idx:])\n",
    "\n",
    "# Convert indices to tensors\n",
    "train_indices5 = torch.tensor(train_indices5)\n",
    "val_indices5 = torch.tensor(val_indices5)\n",
    "\n",
    "# Create Subsets for train and validation datasets\n",
    "train_dataset5 = Subset(tokenized_datasets5, train_indices5)\n",
    "eval_dataset5 = Subset(tokenized_datasets5, val_indices5)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader5 = DataLoader(train_dataset5, shuffle=True, batch_size=12)\n",
    "eval_dataloader5 = DataLoader(eval_dataset5, batch_size=12)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer5 = AdamW(model5.parameters(), lr=5e-6)\n",
    "num_epochs5 = 5\n",
    "num_training_steps5 = num_epochs5 * len(train_dataloader5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer5, num_warmup_steps=int(0.05*num_training_steps5), num_training_steps=num_training_steps5\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device5 = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model5.to(device5)\n",
    "\n",
    "# Initialize lists to track training/validation losses and accuracies\n",
    "train_losses5 = []\n",
    "val_losses5 = []\n",
    "val_f1_scores5 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_4628\\298034166.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/COHERENCE_EPOCH4_F10.4253.pt\n"
     ]
    }
   ],
   "source": [
    "# Specify the file name from which to load the model\n",
    "modelsavename = \"../saved_models/COHERENCE_EPOCH5_F10.4195.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(modelsavename, \"rb\") as f:\n",
    "            model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {modelsavename}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence QWK for Validation Dataset: 0.01657020351825189\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "model5.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model5(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Coherence QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train QWK for Validation Dataset: 0.01081455554608668\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "model5.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model5(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Coherence QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_4628\\2269022431.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lexical_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/Lexical_epoch5.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_saved_model_name = \"../saved_models/Lexical_epoch5.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "lexical_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(lexical_saved_model_name, \"rb\") as f:\n",
    "            lexical_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {lexical_slexical_saved_model_nameaved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "lexical_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Validation Dataset: 0.5029868678667244\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "lexical_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = lexical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Lexical QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "lexical_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = lexical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Lexical QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_4628\\965009692.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  grammatical_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/grammatical_epoch4_F10.5099.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammatical_saved_model_name = \"../saved_models/grammatical_epoch4_F10.5099.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "grammatical_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(lexical_saved_model_name, \"rb\") as f:\n",
    "            grammatical_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {grammatical_saved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "grammatical_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammatical QWK for Validation Dataset: 0.5029868678667244\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "grammatical_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = grammatical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Grammatical QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "grammatical_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = grammatical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Lexical QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Achievement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_4628\\1915021292.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  TA_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/task_achievement_trained.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TA_saved_model_name = \"../saved_models/task_achievement_trained.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "TA_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(lexical_saved_model_name, \"rb\") as f:\n",
    "            TA_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {TA_saved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "TA_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Achievement QWK for Validation Dataset: 0.5029868678667244\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "TA_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = TA_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Task Achievement QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Achievement QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "TA_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = TA_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Task Achievement QWK for Training Dataset: {qwk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
