{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.copy_on_write = True\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_28540\\2055995829.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "fpath = \"../datasets_ready/Task_Achievement.csv\"\n",
    "df5 = pd.read_csv(fpath)\n",
    "df5['score'] = df5['score'].round(1)\n",
    "\n",
    "df_filtered5 = df5[(df5['score'] > 3.0) & (df5['score'] < 12.0)]\n",
    "\n",
    "reverse_mapping_3 = {\n",
    "    3.5: 0, 4.0: 0,\n",
    "    4.5: 1, 5.0: 1,\n",
    "    5.5: 2, 6.0: 2,\n",
    "    6.5: 3, 7.0: 3,\n",
    "    7.5: 4, 8.0: 4,\n",
    "    8.5: 5, 9.0: 5\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "df_filtered5['score'] = df_filtered5['score'].map(reverse_mapping_3)\n",
    "\n",
    "# Sample the maximum available size for each class\n",
    "df_sampled5 = df_filtered5.groupby('score', group_keys=False).apply(\n",
    "    lambda x: x.sample(len(x), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dataset5 = Dataset.from_pandas(df_sampled5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12e300e0dfb4705ad4336285818f0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26597\\miniconda3\\envs\\ece1786\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels_5 = 6\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\", num_labels=num_labels_5, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate the input columns for each example in the batch\n",
    "    combined_text = [\n",
    "        p + \" \" + e + \" \" + t for p, e, t in zip(examples[\"prompt\"], examples[\"essay\"], examples[\"text\"])\n",
    "    ]\n",
    "    # Tokenize the concatenated text\n",
    "    return tokenizer5(combined_text, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets5 = dataset5.map(tokenize_function, batched=True)\n",
    "tokenized_datasets5 = tokenized_datasets5.remove_columns([\"prompt\", \"essay\", \"text\"])\n",
    "tokenized_datasets5 = tokenized_datasets5.rename_column(\"score\", \"labels\")\n",
    "tokenized_datasets5.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "\n",
    "# Get the labels from the tokenized dataset\n",
    "labels5 = tokenized_datasets5[\"labels\"]\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels5 = np.unique(labels5)\n",
    "\n",
    "# Store the indices for each label\n",
    "label_to_indices5 = {label: np.where(labels5 == label)[0] for label in unique_labels5}\n",
    "\n",
    "# Lists to hold the train and validation indices\n",
    "train_indices5 = []\n",
    "val_indices5 = []\n",
    "\n",
    "# For each label, split the indices into train and validation\n",
    "for label, indices in label_to_indices5.items():\n",
    "    # Shuffle the indices within each label to ensure random splitting\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split 80% for training, 20% for validation\n",
    "    split_idx = int(0.8 * len(indices))\n",
    "    train_indices5.extend(indices[:split_idx])\n",
    "    val_indices5.extend(indices[split_idx:])\n",
    "\n",
    "# Convert indices to tensors\n",
    "train_indices5 = torch.tensor(train_indices5)\n",
    "val_indices5 = torch.tensor(val_indices5)\n",
    "\n",
    "# Create Subsets for train and validation datasets\n",
    "train_dataset5 = Subset(tokenized_datasets5, train_indices5)\n",
    "eval_dataset5 = Subset(tokenized_datasets5, val_indices5)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader5 = DataLoader(train_dataset5, shuffle=True, batch_size=12)\n",
    "eval_dataloader5 = DataLoader(eval_dataset5, batch_size=12)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer5 = AdamW(model5.parameters(), lr=5e-6)\n",
    "num_epochs5 = 5\n",
    "num_training_steps5 = num_epochs5 * len(train_dataloader5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer5, num_warmup_steps=int(0.05*num_training_steps5), num_training_steps=num_training_steps5\n",
    ")\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "device5 = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model5.to(device5)\n",
    "\n",
    "# Initialize lists to track training/validation losses and accuracies\n",
    "train_losses5 = []\n",
    "val_losses5 = []\n",
    "val_f1_scores5 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_28540\\298034166.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/COHERENCE_EPOCH4_F10.4253.pt\n"
     ]
    }
   ],
   "source": [
    "# Specify the file name from which to load the model\n",
    "modelsavename = \"../saved_models/COHERENCE_EPOCH4_F10.4253.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(modelsavename, \"rb\") as f:\n",
    "            model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {modelsavename}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence QWK for Validation Dataset: 0.6033783627366514\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Coherence QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence QWK for Training Dataset: 0.4219602258368118\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Coherence QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_28540\\2269022431.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lexical_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/Lexical_epoch5.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_saved_model_name = \"../saved_models/Lexical_epoch5.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "lexical_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(lexical_saved_model_name, \"rb\") as f:\n",
    "            lexical_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {lexical_saved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "lexical_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Validation Dataset: 0.4827286198835399\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "lexical_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = lexical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Lexical QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "lexical_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = lexical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Lexical QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_28540\\2943597027.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  grammatical_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/grammatical_epoch4_F10.5099.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammatical_saved_model_name = \"../saved_models/grammatical_epoch4_F10.5099.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "grammatical_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(grammatical_saved_model_name, \"rb\") as f:\n",
    "            grammatical_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {grammatical_saved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "grammatical_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammatical QWK for Validation Dataset: 0.6124279105810377\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "grammatical_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = grammatical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Grammatical QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "grammatical_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = grammatical_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Grammatical QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Achievement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mrm8488/deberta-v3-ft-financial-news-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\26597\\AppData\\Local\\Temp\\ipykernel_28540\\2615825571.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  TA_model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/task_achievement_trained.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TA_saved_model_name = \"../saved_models/task_achievement_trained.pt\"\n",
    "\n",
    "# Initialize the same model architecture\n",
    "TA_model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\",num_labels=6, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        with open(TA_saved_model_name, \"rb\") as f:\n",
    "            TA_model.load_state_dict(torch.load(f))\n",
    "            print(f\"Model loaded from {TA_saved_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "TA_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Achievement QWK for Validation Dataset: 0.7160319367053913\n"
     ]
    }
   ],
   "source": [
    "all_eval_labels = []\n",
    "all_eval_preds = []\n",
    "TA_model.eval()\n",
    "# len(eval_dataloader5) = 140\n",
    "for batch in eval_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = TA_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_eval_labels.append(labels)\n",
    "    all_eval_preds.append(predictions)\n",
    "all_eval_labels = torch.cat(all_eval_labels).cpu().numpy()\n",
    "all_eval_preds = torch.cat(all_eval_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_eval_labels, all_eval_preds, weights=\"quadratic\")\n",
    "print(f\"Task Achievement QWK for Validation Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Achievement QWK for Training Dataset: 0.4798498176496018\n"
     ]
    }
   ],
   "source": [
    "all_train_labels = []\n",
    "all_train_preds = []\n",
    "TA_model.eval()\n",
    "for batch in train_dataloader5:\n",
    "    batch = {k: v.to(device5) for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = TA_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    all_train_labels.append(labels)\n",
    "    all_train_preds.append(predictions)\n",
    "all_train_labels = torch.cat(all_train_labels).cpu().numpy()\n",
    "all_train_preds = torch.cat(all_train_preds).cpu().numpy()\n",
    "\n",
    "# Compute QWK\n",
    "qwk = cohen_kappa_score(all_train_labels, all_train_preds, weights=\"quadratic\")\n",
    "print(f\"Task Achievement QWK for Training Dataset: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama3.2 1B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Allocated: 0.0 MB\n",
      "Memory Reserved: 0.0 MB\n",
      "Train dataset length: 7840, Test dataset length: 1960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8995d7a117b43228eb37eae6b9f856d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbcdaaadd0c4dd0859c7dd6f88d4cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset columns: ['input_text', 'labels', '__index_level_0__', 'input_ids', 'attention_mask']\n",
      "train_dataset[0]: {'input_text': 'You are a member of the IELTS essay evaluation committee.\\nYour task is to evaluate the essay based on the given prompt and assign it a score\\n        between 4 and 9 (in 1 point increments). 4 and 9 are the lowest and highest scores possible.\\n        Prompt contain the topic of the essay. The essay is the text that you need to evaluate.\\n        Think step by step why this essay is good or bad. \\n        \"Prompt: You can get up-to-date news from the radio, TV and the Internet. Which kind of media do you think is the best to get the news?\\nEssay: Nowadays, there are several channels to get news, such as the radio, TV, newspapers and the Internet. I think the Internet is the best among these. .Since its invention, the Internet has been booming as a prospective industry. Not only because it is a combination of text, audio and video, but also due to its . It has threatened the domination of spreading news of the traditional media, and, I would say, is about to take control.We can find everything we want on the Internet – the latest news, books, songs, movies, cartoons… . With the radio, we can merely hear. Compared to a newspaper, the radio and TV can provide the latest information. For instance the breaking news of the Americans’ attacking Iraq was immediately online, as well as that the Twin Towers of New York were destroyed on September 11, 2001. However, we just cannot .The Internet is a way of getting information, as long as your mobile phone is connected or you possess a laptop. When I am a vehicle, I usually have my cellphone connected to the Internet, then browse through what in the past few hours, or log in MSN to begin a conversation with my friends. Reading a newspaper is also a good way to kill time, but for me, a youngster, it is not so modern as “surfing online while commuting”.The traditional media will never disappear, though the Internet has taken a big advantage the competition. And definitely, there is still a long way for the Internet to go. , the Internet benefits me the most, and I highly appreciate it.', 'labels': 2, '__index_level_0__': 9872, 'input_ids': [128000, 2675, 527, 264, 4562, 315, 279, 358, 2818, 10155, 9071, 16865, 13093, 627, 7927, 3465, 374, 311, 15806, 279, 9071, 3196, 389, 279, 2728, 10137, 323, 9993, 433, 264, 5573, 198, 286, 1990, 220, 19, 323, 220, 24, 320, 258, 220, 16, 1486, 62700, 570, 220, 19, 323, 220, 24, 527, 279, 15821, 323, 8592, 12483, 3284, 627, 286, 60601, 6782, 279, 8712, 315, 279, 9071, 13, 578, 9071, 374, 279, 1495, 430, 499, 1205, 311, 15806, 627, 286, 21834, 3094, 555, 3094, 3249, 420, 9071, 374, 1695, 477, 3958, 13, 720, 286, 330, 55715, 25, 1472, 649, 636, 709, 4791, 18920, 3754, 505, 279, 9063, 11, 6007, 323, 279, 8191, 13, 16299, 3169, 315, 3772, 656, 499, 1781, 374, 279, 1888, 311, 636, 279, 3754, 5380, 85901, 25, 87581, 11, 1070, 527, 3892, 12006, 311, 636, 3754, 11, 1778, 439, 279, 9063, 11, 6007, 11, 32594, 323, 279, 8191, 13, 358, 1781, 279, 8191, 374, 279, 1888, 4315, 1521, 13, 662, 12834, 1202, 28229, 11, 279, 8191, 706, 1027, 68790, 439, 264, 33547, 5064, 13, 2876, 1193, 1606, 433, 374, 264, 10824, 315, 1495, 11, 7855, 323, 2835, 11, 719, 1101, 4245, 311, 1202, 662, 1102, 706, 21699, 279, 55949, 315, 31135, 3754, 315, 279, 8776, 3772, 11, 323, 11, 358, 1053, 2019, 11, 374, 922, 311, 1935, 2585, 23210, 649, 1505, 4395, 584, 1390, 389, 279, 8191, 1389, 279, 5652, 3754, 11, 6603, 11, 11936, 11, 9698, 11, 63123, 1981, 662, 3161, 279, 9063, 11, 584, 649, 16632, 6865, 13, 59813, 311, 264, 17222, 11, 279, 9063, 323, 6007, 649, 3493, 279, 5652, 2038, 13, 1789, 2937, 279, 15061, 3754, 315, 279, 9053, 529, 23664, 11340, 574, 7214, 2930, 11, 439, 1664, 439, 430, 279, 36047, 68457, 315, 1561, 4356, 1051, 14763, 389, 6250, 220, 806, 11, 220, 1049, 16, 13, 4452, 11, 584, 1120, 4250, 662, 791, 8191, 374, 264, 1648, 315, 3794, 2038, 11, 439, 1317, 439, 701, 6505, 4641, 374, 8599, 477, 499, 15575, 264, 21288, 13, 3277, 358, 1097, 264, 7458, 11, 358, 6118, 617, 856, 58409, 8599, 311, 279, 8191, 11, 1243, 27100, 1555, 1148, 304, 279, 3347, 2478, 4207, 11, 477, 1515, 304, 10504, 45, 311, 3240, 264, 10652, 449, 856, 4885, 13, 18242, 264, 17222, 374, 1101, 264, 1695, 1648, 311, 5622, 892, 11, 719, 369, 757, 11, 264, 83371, 11, 433, 374, 539, 779, 6617, 439, 1054, 78960, 287, 2930, 1418, 94950, 11453, 791, 8776, 3772, 690, 2646, 32153, 11, 3582, 279, 8191, 706, 4529, 264, 2466, 9610, 279, 10937, 13, 1628, 8659, 11, 1070, 374, 2103, 264, 1317, 1648, 369, 279, 8191, 311, 733, 13, 1174, 279, 8191, 7720, 757, 279, 1455, 11, 323, 358, 7701, 15763, 433, 13, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
      "train_dataset['labels'][0]: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "# Clear cache\n",
    "\n",
    "# For debugging purposes, check memory stats\n",
    "print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6} MB\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "llama3_model_path = \"meta-llama/Llama-3.2-1B\"\n",
    "file_path = \"../datasets_ready/combined_dataset.csv\"\n",
    "checkpoint_dir = \"../results/checkpoint-4410\"\n",
    "SAMPLE_SIZE = 9800\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def create_input_text(row):\n",
    "    return (\n",
    "        f\"\"\"You are a member of the IELTS essay evaluation committee.\\nYour task is to evaluate the essay based on the given prompt and assign it a score\n",
    "        between 4 and 9 (in 1 point increments). 4 and 9 are the lowest and highest scores possible.\n",
    "        Prompt contain the topic of the essay. The essay is the text that you need to evaluate.\n",
    "        Think step by step why this essay is good or bad. \n",
    "        \"Prompt: {row['prompt']}\\nEssay: {row['essay']}\"\"\"\n",
    "    )\n",
    "\n",
    "def map_band_to_class(band):\n",
    "    if band ==  \"<4\":\n",
    "        return band_to_class['<4']\n",
    "    return band_to_class[str(band)]\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "band_classes = ['<4'] + ['4', '4.5', '5', '5.5', '6', '6.5', '7', '7.5', '8', '8.5', '9'] \n",
    "# class_to_band = {i: band for band, i in band_to_class.items()}  # Class → Band\n",
    "\n",
    "band_to_class = {\n",
    "    '<4': 0, '4': 0,\n",
    "    '4.5': 1, '5': 1,\n",
    "    '5.5': 2, '6': 2,\n",
    "    '6.5': 3, '7': 3,\n",
    "    '7.5': 4, '8': 4,\n",
    "    '8.5': 5, '9': 5\n",
    "}\n",
    "\n",
    "# Map bands to classes\n",
    "def map_band_to_class(band):\n",
    "    if band ==  \"<4\":\n",
    "        return band_to_class['<4']\n",
    "    return band_to_class[str(band)]\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "data = pd.read_csv(file_path).sample(n=SAMPLE_SIZE, random_state=42)\n",
    "data['input_text'] = data.apply(create_input_text, axis=1)\n",
    "data['labels'] = data['band'].apply(map_band_to_class)\n",
    "# Drop unnecessary columns. Might need them later\n",
    "data = data.drop(columns=[\"evaluation\", \"band\", \"prompt\", \"essay\"])\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Prepare datasets for Hugging Face Trainer\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama3_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "print(f\"train_dataset columns: {train_dataset.column_names}\")\n",
    "print(f\"train_dataset[0]: {train_dataset[0]},\\ntrain_dataset['labels'][0]: {train_dataset['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "num_labels = 6  # Total number of unique band scores\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "t = model.config.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Freeze the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26597\\miniconda3\\envs\\ece1786\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3f4be5b38a49fa942b8622fe950301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.6765220165252686, 'eval_model_preparation_time': 0.004, 'eval_accuracy': 0.29846938775510207, 'eval_QWK': 0.3236651161035259, 'eval_MAE': 1.1418367346938776, 'eval_runtime': 784.3186, 'eval_samples_per_second': 2.499, 'eval_steps_per_second': 0.157}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    ")\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    qwk = cohen_kappa_score(labels, preds, weights=\"quadratic\")\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"QWK\": qwk,\n",
    "        \"MAE\": mae,\n",
    "    }\n",
    "    \n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "test_results = trainer.evaluate()\n",
    "print(f\"Test Results: {test_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
